// REQUIRES: TARGET-GCUALL
// RUN: choreo -gs -t factor %s -o %s.result && bash %s.result --execute | FileCheck --match-full-lines %s && rm -f %s.result
// RUN: choreo -gs -t topscc %s -o %s.topscc.result && bash %s.topscc.result --execute | FileCheck --match-full-lines %s && rm -f %s.topscc.result
// RUN-DOCKER: choreo -st -t topscc %s -o %s.result && %s.result | FileCheck --match-full-lines %s && rm -f %s.result

#include "choreo.h"
#include <algorithm>
#include <cfloat>
#include <cmath>
#include <iostream>
#include <vector>

__cok__ { /// kernel program

__co_device__  extern "C" void matmul_kernel(float *lhs, float *rhs, float *out, int m,
                                int k, int n) {
    for (int i = 0; i < m; ++i)
      for (int j = 0; j < n; ++j)
        for (int z = 0; z < k; ++z)
          out[i * n + j] += lhs[i * k + z] * rhs[z * n + j];
  }

__co_device__  extern "C" void softmax_kernel(float *input, float *output, int m, int n) {
    for (int i = 0; i < m; ++i) {
      float max_val = input[i * n] / 22.627417;
      // Find the maximum value in the current row (to avoid numerical overflow)
      for (int j = 0; j < n; ++j) {
        input[i * n + j] /= 22.627417;
        max_val = (max_val > input[i * n + j]) ? max_val : input[i * n + j];
      }
      float sum = 0.0f;
      // Compute the exp of each element and accumulate it in sum
      for (int j = 0; j < n; ++j) {
        output[i * n + j] = expf(input[i * n + j] - max_val);
        sum += output[i * n + j];
      }

      // Normalize each element by dividing it by sum
      for (int j = 0; j < n; ++j)
        output[i * n + j] /= sum;
    }
  }
__co_device__ extern "C" void matmul_kernel_ext(float *lhs, float *rhs, float *out, float *max_row_values, int m, int k, int n) {
    for (int i = 0; i < m; ++i) {
        float max_val = -100; // Initialize with the smallest possible value
        for (int j = 0; j < n; ++j) {
            out[i * n + j] = 0.0f; // Initialize the output value
            for (int z = 0; z < k; ++z) {
                out[i * n + j] += lhs[i * k + z] * rhs[z * n + j];
            }
            // Track the maximum value in the current row
            if (out[i * n + j] > max_val) {
                max_val = out[i * n + j];
            }
        }
        // Store the maximum value of the current row in max_row_values
        max_row_values[i] = max_val;
    }
}
__co_device__  extern "C" void softmax_kernel_ext(float *input, float *output, float *max_row_values, int m, int n) {
    for (int i = 0; i < m; ++i) {
      float sum = 0.0f;
      // Compute the exp of each element and accumulate it in sum
      for (int j = 0; j < n; ++j) {
        output[i * n + j] = expf((input[i * n + j] - max_row_values[i])/22.627417);
        sum += output[i * n + j];
      }

      // Normalize each element by dividing it by sum
      for (int j = 0; j < n; ++j)
        output[i * n + j] /= sum;
    }
  }

} /// end of kernel decl

__co__ f32[128, 512] math_product_attention(f32[128, 512] lhs,
                                            f32[512, 128] rhs,
                                            f32[128, 512] v) {

  f32[lhs.span(0), v.span(1)] output;
  parallel p by 1 {
    shared f32[lhs.span(0), rhs.span(1)] l2_out;
    local f32[4] maxval{0};
    with index = {m_tile, k_tile,
                  n_tile} in[32, 1, 8] { // decl your tiled spans
      foreach
        m_tile, n_tile {
          local f32[4, 16] l1_out{0};
          foreach
            k_tile {
              lhs_load = dma.copy lhs.chunkat(m_tile, k_tile) => local;
              rhs_load = dma.copy rhs.chunkat(k_tile, n_tile) => local;

              call matmul_kernel_ext(lhs_load.data, rhs_load.data, maxval,l1_out, 4, 512,
                                 16);
              dma.copy l1_out => l2_out.chunkat(m_tile, n_tile);
            }
        }
    }
    with index = {m_tile1, n_tile1} in[2, 1] { // Define tile size
      foreach
        m_tile1 {
          local f32[64, 128] local_output;

          // Use DMA to load tile data into local memory
          local_input = dma.copy l2_out.chunkat(m_tile1, n_tile1) => local;

          // Call the softmax kernel function
          call softmax_kernel_ext(local_input.data, local_output,maxval  ,64, 128);

          // Copy the result back to global memory
          dma.copy local_output => l2_out.chunkat(m_tile1, n_tile1);
        }
    }
    with index = {m_tile2, k_tile2,
                  n_tile2} in[32, 1, 8] { // decl your tiled spans
      foreach
        m_tile2, n_tile2 {
          local f32[4, 64] l1_out_v{0};
          foreach
            k_tile2 {
              lhs_load_v = dma.copy l2_out.chunkat(m_tile2, k_tile2) => local;
              rhs_load_v = dma.copy v.chunkat(k_tile2, n_tile2) => local;

              call matmul_kernel(lhs_load_v.data, rhs_load_v.data, l1_out_v, 4,
                                 128, 64);
              dma.copy l1_out_v => output.chunkat(m_tile2, n_tile2);
            }
        }
    }
  }
  return output;
}
// Function to calculate dot product of two vectors
extern "C" float dot_product(const float *a, const float *b, int size) {
  float result = 0.0;
  for (int i = 0; i < size; ++i) {
    result += a[i] * b[i];
  }
  return result;
}

// Function to calculate softmax of a vector
extern "C" void softmax(float *vector, int size) {
  float sum = 0.0;
  float maxv = FLT_MIN;
  for (int i = 0; i < size; ++i) {
    maxv = (maxv > vector[i]) ? maxv : vector[i];
  }
  for (int i = 0; i < size; ++i) {
    vector[i] = exp(vector[i] - maxv);
    sum += vector[i];
  }
  for (int i = 0; i < size; ++i) {
    vector[i] /= sum;
  }
}

// Function to perform dot-product attention
extern "C" void cpu_attention(const float *query, const float *key,
                              const float *value, float *output, int seq_len,
                              int embedding_dim) {

  float *temp = (float *)malloc(sizeof(float) * seq_len * seq_len);
  float max_attention = FLT_MIN;
  // Calculate attention scores
  for (int i = 0; i < seq_len; ++i) {
    for (int j = 0; j < seq_len; ++j) {
      float attention_score =
          (dot_product(&query[i * embedding_dim], &key[j * embedding_dim],
                       embedding_dim)) /
          sqrt((float)embedding_dim);
      temp[i * seq_len + j] = attention_score;
    }
  }

  // Apply softmax to get attention weights
  for (int i = 0; i < seq_len; ++i) {
    softmax(&temp[i * seq_len], seq_len);
  }

  // Calculate the weighted sum of values using attention weights
  for (int i = 0; i < seq_len; ++i) {
    for (int j = 0; j < embedding_dim; ++j) {
      output[i * embedding_dim + j] = 0.0;
      for (int k = 0; k < seq_len; ++k) {
        output[i * embedding_dim + j] +=
            temp[i * seq_len + k] * value[k * embedding_dim + j];
      }
    }
  }
  free(temp);
}
int main() {
  // Define matrix sizes
  int m = 128; // Rows in Q and output
  int n = 512; // Columns in K and V (and output)
  int k = 512; // Columns in Q and rows in K

  // Allocate memory for Q, K, V
  choreo::f32 key[128][512]; // Query matrix (128 x 512)
  choreo::f32 q[512][128];   // Key matrix (512 x 512)
  choreo::f32 v[128][512];   // Value matrix (512 x 512)

  // Initialize Q, K, V with some values (example)
  std::fill(&key[0][0], &key[0][0] + 128 * 512, 1.0f); // Fill lhs (Q) with 1.0
  std::fill(&q[0][0], &q[0][0] + 512 * 128, 1.0f);     // Fill rhs (K) with 1.0
  std::fill(&v[0][0], &v[0][0] + 128 * 512, 1.0f);     // Fill v (V) with 1.0
  for (int i = 0; i < 128; ++i) {
    for (int j = 0; j < 512; ++j) {
      key[i][j] = i + j; // Fill lhs (Q) with unique values
      q[i][j] = i + j;   // Fill rhs (K) with unique values
      v[i][j] = i + j;   // Fill v (V) with unique values
    }
  }
  auto k_data =
      choreo::make_spanview<2, choreo::f32>((choreo::f32 *)key, {128, 512});
  auto q_data =
      choreo::make_spanview<2, choreo::f32>((choreo::f32 *)q, {512, 128});
  auto v_data =
      choreo::make_spanview<2, choreo::f32>((choreo::f32 *)v, {128, 512});

  // Call attention function
  auto result = math_product_attention(k_data, q_data, v_data);
  float cpu_result[128][512] = {0};
  // Call attention function on CPU
  cpu_attention(&q[0][0], &key[0][0], &v[0][0], &cpu_result[0][0], 128, 512);

  // Compare results
  bool match = true;
  for (int i = 0; i < m; ++i) {
    for (int j = 0; j < n; ++j) {
      if (fabs(result[i][j] - cpu_result[i][j]) > 1e-4) {
        match = false;
        std::cout << "Mismatch at (" << i << ", " << j
                  << "): GCU = " << result[i][j]
                  << ", CPU = " << cpu_result[i][j] << std::endl;
      }
    }
  }

  if (match) {
    std::cout << "Attention Test Passed\n" << std::endl;
  } else {
    std::cout << "Attention Test Failed\n" << std::endl;
  }

  return 0;
}
// CHECK: Attention Test Passed

