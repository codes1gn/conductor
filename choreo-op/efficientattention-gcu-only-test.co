// REQUIRES: TARGET-GCUALL
// RUN: choreo -gs -t factor %s -o %s.factor.result && bash %s.factor.result --execute | FileCheck  --match-full-lines %s && rm -f %s.factor.result
// RUN: choreo -gs -t topscc %s -o %s.topscc.result && bash %s.topscc.result --execute | FileCheck  --match-full-lines %s && rm -f %s.topscc.result

#include <algorithm>
#include <cfloat>
#include <cmath>
#include <iostream>
#include <vector>

__cok__ { /// kernel program

__co_device__  extern "C" void matmul_kernel(float* lhs, float* rhs, float* out, int m,
                                int k, int n) {
    for (int i = 0; i < m; ++i)
      for (int j = 0; j < n; ++j)
        for (int z = 0; z < k; ++z)
          out[i * n + j] += lhs[i * k + z] * rhs[z * n + j];
  }

__co_device__  extern "C" void softmax_kernel(float* input, float* output, int m, int n) {
    for (int i = 0; i < m; ++i) {
      float max_val = input[i * n] / 22.627417;
      // Find the maximum value in the current row (to avoid numerical overflow)
      for (int j = 0; j < n; ++j) {
        input[i * n + j] /= 22.627417;
        max_val = (max_val > input[i * n + j]) ? max_val : input[i * n + j];
      }
      float sum = 0.0f;
      // Compute the exp of each element and accumulate it in sum
      for (int j = 0; j < n; ++j) {
        output[i * n + j] = expf(input[i * n + j] - max_val);
        sum += output[i * n + j];
      }

      // Normalize each element by dividing it by sum
      for (int j = 0; j < n; ++j) output[i * n + j] /= sum;
    }
  }
__co_device__  extern "C" void matmul_kernel_ext(float* lhs, float* rhs, float* out,
                                    float* max_row_values, int m, int k,
                                    int n) {
    for (int i = 0; i < m; ++i) {
      float max_val = -100; // Initialize with the smallest possible value
      for (int j = 0; j < n; ++j) {
        out[i * n + j] = 0.0f; // Initialize the output value
        for (int z = 0; z < k; ++z) {
          out[i * n + j] += lhs[i * k + z] * rhs[z * n + j];
        }
        // Track the maximum value in the current row
        if (out[i * n + j] > max_val) { max_val = out[i * n + j]; }
      }
      // Store the maximum value of the current row in max_row_values
      max_row_values[i] = max_val;
    }
  }
__co_device__  extern "C" void softmax_kernel_ext(float* input, float* output,
                                     float* max_row_values, int m, int n) {
    for (int i = 0; i < m; ++i) {
      float sum = 0.0f;
      // Compute the exp of each element and accumulate it in sum
      for (int j = 0; j < n; ++j) {
        output[i * n + j] =
            expf((input[i * n + j] - max_row_values[i]) / 22.627417);
        sum += output[i * n + j];
      }

      // Normalize each element by dividing it by sum
      for (int j = 0; j < n; ++j) output[i * n + j] /= sum;
    }
  }

} /// end of kernel decl
__co__ f32[128, 256] efficient_attention(f32[128, 256] lhs, f32[256, 128] rhs,
                                         f32[128, 256] v) {

  f32[lhs.span(0), v.span(1)] output;
  parallel p by 1 {
    shared f32[lhs.span(0), rhs.span(1)] l2_out;
    local f32[4] maxval{0};
    with index = {m_tile, k_tile,
                  n_tile} in[32, 1, 8] { // decl your tiled spans
      foreach
        m_tile {
          foreach
            n_tile {
              local f32[4, 16] l1_out{0};
              foreach
                k_tile {
                  lhs_load = dma.copy lhs.chunkat(m_tile, k_tile) => local;
                  rhs_load = dma.copy rhs.chunkat(k_tile, n_tile) => local;

                  call matmul_kernel_ext(lhs_load.data, rhs_load.data, maxval,
                                         l1_out, 4, 256, 32);
                  dma.copy l1_out => l2_out.chunkat(m_tile, n_tile);
                }
            }
          local f32[4, 16] local_output;

          // Use DMA to load tile data into local memory
          local_input = dma.copy l2_out.chunkat(m_tile, n_tile) => local;

          // Call the softmax kernel function
          call softmax_kernel_ext(local_input.data, local_output, maxval, 4,
                                  16);

          // Copy the result back to global memory
          dma.copy local_output => l2_out.chunkat(m_tile, n_tile);
        }
      foreach
        m_tile, n_tile {
          local f32[4, 32] l1_out_v{0};
          foreach
            k_tile {
              lhs_load_v = dma.copy l2_out.chunkat(m_tile, k_tile) => local;
              rhs_load_v = dma.copy v.chunkat(k_tile, n_tile) => local;

              call matmul_kernel(lhs_load_v.data, rhs_load_v.data, l1_out_v, 4,
                                 128, 32);
              dma.copy l1_out_v => output.chunkat(m_tile, n_tile);
            }
        }
    }
  }
  return output;
}
// Function to calculate dot product of two vectors
extern "C" float dot_product(const float* a, const float* b, int size) {
  float result = 0.0;
  for (int i = 0; i < size; ++i) { result += a[i] * b[i]; }
  return result;
}

// Function to calculate softmax of a vector
extern "C" void softmax(float* vector, int size) {
  float sum = 0.0;
  float maxv = FLT_MIN;
  for (int i = 0; i < size; ++i) {
    maxv = (maxv > vector[i]) ? maxv : vector[i];
  }
  for (int i = 0; i < size; ++i) {
    vector[i] = exp(vector[i] - maxv);
    sum += vector[i];
  }
  for (int i = 0; i < size; ++i) { vector[i] /= sum; }
}

// Function to perform dot-product attention
extern "C" void cpu_attention(const float* query, const float* key,
                              const float* value, float* output, int seq_len,
                              int embedding_dim) {

  float* temp = (float*)malloc(sizeof(float) * seq_len * seq_len);
  float max_attention = FLT_MIN;
  // Calculate attention scores
  for (int i = 0; i < seq_len; ++i) {
    for (int j = 0; j < seq_len; ++j) {
      float attention_score =
          (dot_product(&query[i * embedding_dim], &key[j * embedding_dim],
                       embedding_dim)) /
          sqrt((float)embedding_dim);
      temp[i * seq_len + j] = attention_score;
    }
  }

  // Apply softmax to get attention weights
  for (int i = 0; i < seq_len; ++i) { softmax(&temp[i * seq_len], seq_len); }

  // Calculate the weighted sum of values using attention weights
  for (int i = 0; i < seq_len; ++i) {
    for (int j = 0; j < embedding_dim; ++j) {
      output[i * embedding_dim + j] = 0.0;
      for (int k = 0; k < seq_len; ++k) {
        output[i * embedding_dim + j] +=
            temp[i * seq_len + k] * value[k * embedding_dim + j];
      }
    }
  }
  free(temp);
}
int main() {
  // Define matrix sizes
  int m = 128; // Rows in Q and output
  int n = 256; // Columns in K and V (and output)
  int k = 256; // Columns in Q and rows in K

  // Allocate memory for Q, K, V
  choreo::f32 key[128][256]; // Query matrix (128 x 256)
  choreo::f32 q[256][128];   // Key matrix (256 x 256)
  choreo::f32 v[128][256];   // Value matrix (256 x 256)

  // Initialize Q, K, V with some values (example)
  std::fill(&key[0][0], &key[0][0] + 128 * 256, 1.0f); // Fill lhs (Q) with 1.0
  std::fill(&q[0][0], &q[0][0] + 256 * 128, 1.0f);     // Fill rhs (K) with 1.0
  std::fill(&v[0][0], &v[0][0] + 128 * 256, 1.0f);     // Fill v (V) with 1.0
  for (int i = 0; i < 128; ++i) {
    for (int j = 0; j < 256; ++j) {
      key[i][j] = i + j; // Fill lhs (Q) with unique values
      q[i][j] = i + j;   // Fill rhs (K) with unique values
      v[i][j] = i + j;   // Fill v (V) with unique values
    }
  }
  auto k_data =
      choreo::make_spanview<2, choreo::f32>((choreo::f32*)key, {128, 256});
  auto q_data =
      choreo::make_spanview<2, choreo::f32>((choreo::f32*)q, {256, 128});
  auto v_data =
      choreo::make_spanview<2, choreo::f32>((choreo::f32*)v, {128, 256});

  // Call attention function
  auto result = efficient_attention(k_data, q_data, v_data);
  float cpu_result[128][256] = {0};
  // Call attention function on CPU
  cpu_attention(&q[0][0], &key[0][0], &v[0][0], &cpu_result[0][0], 128, 256);

  // Compare results
  bool match = true;
  for (int i = 0; i < m; ++i) {
    for (int j = 0; j < n; ++j) {
      if (fabs(result[i][j] - cpu_result[i][j]) > 1e-4) {
        match = false;
        std::cout << "Mismatch at (" << i << ", " << j
                  << "): GCU = " << result[i][j]
                  << ", CPU = " << cpu_result[i][j] << std::endl;
      }
    }
  }

  if (match) {
    std::cout << "Attention Test Passed\n" << std::endl;
  } else {
    std::cout << "Attention Test Failed\n" << std::endl;
  }

  return 0;
}
// CHECK: Attention Test Passed
