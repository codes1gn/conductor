// File: samples/fused/add_relu.co
// Description: Fused addition and ReLU activation
// Source: torch.relu(torch.add(input1, input2))
// Optimizations: Elementwise fusion, intermediate buffer elimination

#include "choreo.h"

__co__ f32 [32, 64] fused_add_relu(f32 [32, 64] input1, f32 [32, 64] input2) {
  f32[input1.span(0), input1.span(1)] output;

  parallel p by 2 {
    with index = {m_tile, n_tile} in [4, 8] {
      foreach m_tile, n_tile {
        lhs_load = dma.copy.async input1.chunkat(p, m_tile, n_tile) => local;
        rhs_load = dma.copy.async input2.chunkat(p, m_tile, n_tile) => local;
        wait lhs_load, rhs_load;

        local f32 [lhs_load.span] l1_out;

        // Fused add + relu operation - no intermediate buffer needed
        foreach i in [l1_out.span] {
          f32 temp_val = lhs_load.data.at(i) + rhs_load.data.at(i);
          l1_out.at(i) = temp_val > 0.0 ? temp_val : 0.0;
        }

        dma.copy l1_out => output.chunkat(p, m_tile, n_tile);
      }
    }
  }

  return output;
}