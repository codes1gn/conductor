// File: samples/complex/attention.co
// Description: Multi-head attention mechanism
// Source: torch.nn.MultiheadAttention forward pass
// Optimizations: Attention pattern fusion, memory layout optimization

#include "choreo.h"

__co__ f32 [8, 128, 512] multihead_attention(
    f32 [8, 128, 512] query,
    f32 [8, 128, 512] key, 
    f32 [8, 128, 512] value
) {
  f32[query.span(0), query.span(1), query.span(2)] output;
  
  parallel p by 2 {
    with index = {b_tile, s_tile, h_tile} in [2, 8, 8] {
      foreach b_tile, s_tile {
        // Load query, key, value chunks
        q_load = dma.copy.async query.chunkat(p, b_tile, s_tile) => shared;
        k_load = dma.copy.async key.chunkat(p, b_tile, s_tile) => shared;
        v_load = dma.copy.async value.chunkat(p, b_tile, s_tile) => shared;
        wait q_load, k_load, v_load;
        
        // Attention computation in local memory
        parallel q by 8 { // num_heads
          with index = {head_tile} in [1] {
            foreach head_tile {
              // Extract head-specific data
              q_head_load = dma.copy.async q_load.data.chunkat(q, head_tile) => local;
              k_head_load = dma.copy.async k_load.data.chunkat(q, head_tile) => local;
              v_head_load = dma.copy.async v_load.data.chunkat(q, head_tile) => local;
              wait q_head_load, k_head_load, v_head_load;
              
              // Local attention computation
              local f32[q_head_load.span(0), k_head_load.span(0)] scores;
              local f32[q_head_load.span(0), v_head_load.span(1)] attended;
              
              // Compute attention scores: Q * K^T
              foreach {i, j, k} in [q_head_load.span(0), k_head_load.span(0), q_head_load.span(1)]
                scores.at(i, j) += q_head_load.data.at(i, k) * k_head_load.data.at(j, k);
              
              // Apply scaling
              f32 scale = 1.0 / sqrt(64.0); // head_dim = 64
              foreach {i, j} in [scores.span(0), scores.span(1)]
                scores.at(i, j) *= scale;
              
              // Softmax (simplified)
              foreach i in [scores.span(0)] {
                f32 max_val = scores.at(i, 0);
                foreach j in [scores.span(1)]
                  max_val = max_val > scores.at(i, j) ? max_val : scores.at(i, j);
                
                f32 sum_exp = 0.0;
                foreach j in [scores.span(1)] {
                  scores.at(i, j) = exp(scores.at(i, j) - max_val);
                  sum_exp += scores.at(i, j);
                }
                
                foreach j in [scores.span(1)]
                  scores.at(i, j) /= sum_exp;
              }
              
              // Apply attention to values: Attention * V
              foreach {i, j, k} in [attended.span(0), attended.span(1), scores.span(1)]
                attended.at(i, j) += scores.at(i, k) * v_head_load.data.at(k, j);
              
              // Store back to shared memory
              attended_store = dma.copy.async attended => q_load.data.chunkat(q, head_tile);
              wait attended_store;
            }
          }
        }
        
        // Store final result
        output_store = dma.copy.async q_load.data => output.chunkat(p, b_tile, s_tile);
        wait output_store;
      }
    }
  }
  
  return output;
}